{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep staging on the Sleep Physionet dataset\n",
    "\n",
    "In this tutorial, we will learn how to train a convolutional neural network on raw EEG data to classify sleep stages.\n",
    "\n",
    "This tutorial is based on the [MNE-Python](https://mne.tools/stable/auto_tutorials/sample-datasets/plot_sleep.html) and [braindecode](https://braindecode.org/auto_examples/plot_sleep_staging.html) sleep staging examples, the [`mne-torch`](https://github.com/mne-tools/mne-torch) repository, as well as\n",
    "\n",
    "> Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., & Gramfort, A. (2018). A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(4), 758-769.\n",
    "\n",
    "## Sleep staging\n",
    "\n",
    "Sleep staging is the process of identifying the sleep stage someone is in by analyzing their EEG and other physiological signals. Sleep recordings are traditionally divided into 30-s windows, and one of five categories (\"stages\") is attributed to each window:\n",
    "\n",
    "1. W: wakefulness\n",
    "2. N1: light sleep\n",
    "3. N2: deeper sleep\n",
    "4. N3: deep sleep\n",
    "5. R: rapid eye movement\n",
    "\n",
    "Sleep staging usually relies on capturing changes in the spectral properties of the EEG as well as transient events (e.g., sleep spindles, k-complexes, slow waves, etc.) that occur under the different sleep stages.\n",
    "\n",
    "In this tutorial, we will train a convolutional neural network (ConvNet) to perform sleep staging on unseen raw EEG. We will use the [Sleep Physionet](https://physionet.org/content/sleep-edfx/1.0.0/) dataset, which contains 153 overnight sleep recordings from 78 individuals. These recordings were manually staged by sleep experts, providing us with the required classification targets to train and evaluate our ConvNet on.\n",
    "\n",
    "## Objective of the tutorial\n",
    "\n",
    "This tutorial is meant to be a general hands-on introduction to training neural networks on EEG data. With this in mind, the default training hyperparameters below are set such that computations are not too long.\n",
    "\n",
    "Once you have been through the whole material though, you are encouraged to experiment with the different elements of the pipeline (e.g., data preprocessing, neural network architecture, optimization parameters, etc.) to try to improve performance as much as possible.\n",
    "\n",
    "## Steps\n",
    "\n",
    "This notebook is divided into the following sections:\n",
    "\n",
    "0. [Set up environment](#0.-Setting-up-the-environment)\n",
    "1. [Load data](#1.-Loading-data)\n",
    "2. [Preprocess data (filter, window)](#2.-Preprocessing-raw-data)\n",
    "3. [Make splits](3.-Making-train/valid/test-splits)\n",
    "4. [Create model](4.-Creating-the-neural-network)\n",
    "5. [Train and monitor](5.-Train-and-monitor-network)\n",
    "6. [Visualize results](#6.-Visualizing-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from mne) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from mne) (1.19.3)\n",
      "Requirement already satisfied: matplotlib in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (1.19.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: six in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: torch in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (1.7.0+cpu)\n",
      "Requirement already satisfied: future in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: typing-extensions in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from torch) (1.19.3)\n",
      "Requirement already satisfied: scikit-learn in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from scikit-learn) (1.19.3)\n",
      "Requirement already satisfied: pandas in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from pandas) (1.19.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/hubert/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for colab\n",
    "!pip install mne\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general modules\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Training will be carried out on CPU, which might be slower.\n",
      "\n",
      "If running on Google Colab, you can request a GPU runtime by clicking\n",
      "`Runtime/Change runtime type` in the top bar menu, then selecting 'GPU'\n",
      "under 'Hardware accelerator'.\n"
     ]
    }
   ],
   "source": [
    "# Identifying whether a CUDA-enabled GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('CUDA-enabled GPU found. Training should be faster.')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No GPU found. Training will be carried out on CPU, which might be '\n",
    "          'slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by'\n",
    "          ' clicking\\n`Runtime/Change runtime type` in the top bar menu, then '\n",
    "          'selecting \\'GPU\\'\\nunder \\'Hardware accelerator\\'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "We start by loading the raw EEG recordings from the Sleep Physionet dataset. MNE-Python already contains a function `fetch_data` which downloads the recordings locally. We then need to read each file from disk.\n",
    "\n",
    "To make the first pass through this tutorial faster, we only load 21 recordings first. Once you were able to run the whole tutorial and are ready to work on improving the performance of the model, you can try loading more subjects and recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne.datasets.sleep_physionet.age import fetch_data\n",
    "\n",
    "mne.set_log_level('ERROR')  # To avoid flooding the cell outputs with messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = range(21)\n",
    "recordings = [1]\n",
    "\n",
    "# To load all subjects and recordings, uncomment the next line\n",
    "# subjects, recordings = range(83), [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d187dfdfdf48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecording\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecordings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-456>\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(subjects, recording, path, force_update, update_path, base_url, on_missing, verbose)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages/mne/datasets/sleep_physionet/age.py\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(subjects, recording, path, force_update, update_path, base_url, on_missing, verbose)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsg_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpsg_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'record'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 psg_fname = _fetch_one(psg_records['fname'][idx].decode(),\n\u001b[0m\u001b[1;32m    123\u001b[0m                                        \u001b[0mpsg_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                                        *params)\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages/mne/datasets/sleep_physionet/_utils.py\u001b[0m in \u001b[0;36m_fetch_one\u001b[0;34m(fname, hashsum, path, force_update, base_url)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         _fetch_file(url, destination, print_destination=False,\n\u001b[0m\u001b[1;32m     39\u001b[0m                     hash_=hashsum, hash_type='sha1')\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-3>\u001b[0m in \u001b[0;36m_fetch_file\u001b[0;34m(url, file_name, print_destination, resume, hash_, timeout, hash_type, verbose)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages/mne/utils/fetching.py\u001b[0m in \u001b[0;36m_fetch_file\u001b[0;34m(url, file_name, print_destination, resume, hash_, timeout, hash_type, verbose)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0minitial_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0m_get_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# check hash sum eg md5sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/site-packages/mne/utils/fetching.py\u001b[0m in \u001b[0;36m_get_http\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl-eeg-tutorial/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fnames = fetch_data(subjects=subjects, recording=recordings, on_missing='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sleep_physionet_raw(raw_fname, annot_fname, load_eeg_only=True, \n",
    "                             crop_wake_mins=30):\n",
    "    \"\"\"Load a recording from the Sleep Physionet dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_fname : str\n",
    "        Path to the .edf file containing the raw data.\n",
    "    annot_fname : str\n",
    "        Path to the annotation file.\n",
    "    load_eeg_only : bool\n",
    "        If True, only keep EEG channels and discard other modalities \n",
    "        (speeds up loading).\n",
    "    crop_wake_mins : float\n",
    "        Number of minutes of wake events before and after sleep events.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mne.io.Raw :\n",
    "        Raw object containing the EEG and annotations.        \n",
    "    \"\"\"\n",
    "    mapping = {'EOG horizontal': 'eog',\n",
    "               'Resp oro-nasal': 'misc',\n",
    "               'EMG submental': 'misc',\n",
    "               'Temp rectal': 'misc',\n",
    "               'Event marker': 'misc'}\n",
    "    exclude = mapping.keys() if load_eeg_only else ()\n",
    "    \n",
    "    raw = mne.io.read_raw_edf(raw_fname, exclude=exclude)\n",
    "    annots = mne.read_annotations(annot_fname)\n",
    "    raw.set_annotations(annots, emit_warning=False)\n",
    "    if not load_eeg_only:\n",
    "        raw.set_channel_types(mapping)\n",
    "    \n",
    "    if crop_wake_mins > 0:  # Cut start and end Wake periods\n",
    "        # Find first and last sleep stages\n",
    "        mask = [x[-1] in ['1', '2', '3', '4', 'R'] \n",
    "                for x in annots.description]\n",
    "        sleep_event_inds = np.where(mask)[0]\n",
    "\n",
    "        # Crop raw\n",
    "        tmin = annots[int(sleep_event_inds[0])]['onset'] - \\\n",
    "               crop_wake_mins * 60\n",
    "        tmax = annots[int(sleep_event_inds[-1])]['onset'] + \\\n",
    "               crop_wake_mins * 60\n",
    "        raw.crop(tmin=tmin, tmax=tmax)\n",
    "    \n",
    "    # Rename EEG channels\n",
    "    ch_names = {i: i.replace('EEG ', '') \n",
    "                for i in raw.ch_names if 'EEG' in i}\n",
    "    mne.rename_channels(raw.info, ch_names)\n",
    "    \n",
    "    # Save subject and recording information in raw.info\n",
    "    basename = os.path.basename(raw_fname)\n",
    "    subj_nb, rec_nb = int(basename[3:5]), int(basename[5])\n",
    "    raw.info['subject_info'] = {'id': subj_nb, 'rec_id': rec_nb}\n",
    "   \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recordings\n",
    "raws = [load_sleep_physionet_raw(f[0], f[1]) for f in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a recording as sanity check\n",
    "raws[0].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing raw data\n",
    "\n",
    "Next, we need to preprocess the raw data. Here, we use a simple filtering step, followed by the extration of 30-s windows. \n",
    "\n",
    "Sleep EEG data has most of its relevant information below 30 Hz. Therefore, to mitigate the impact of higher frequency noise, we apply a lowpass filter with cutoff frequency of 30 Hz to our recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_freq, h_freq = None, 30\n",
    "\n",
    "for raw in raws:\n",
    "    raw.load_data().filter(l_freq, h_freq)  # filtering happens in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the power spectrum of a recording as sanity check\n",
    "raws[0].plot_psd();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to extracting 30-s windows (also called *epochs*) from the filtered data, we define a few functions that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epochs(raw, chunk_duration=30.):\n",
    "    \"\"\"Extract non-overlapping epochs from raw data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "        Raw data object to be windowed.\n",
    "    chunk_duration : float\n",
    "        Length of a window.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Epoched data, of shape (n_epochs, n_channels, n_times).\n",
    "    np.ndarray\n",
    "        Event identifiers for each epoch, shape (n_epochs,).\n",
    "    \"\"\"\n",
    "    annotation_desc_2_event_id = {\n",
    "        'Sleep stage W': 1,\n",
    "        'Sleep stage 1': 2,\n",
    "        'Sleep stage 2': 3,\n",
    "        'Sleep stage 3': 4,\n",
    "        'Sleep stage 4': 4,\n",
    "        'Sleep stage R': 5}\n",
    "\n",
    "    events, _ = mne.events_from_annotations(\n",
    "        raw, event_id=annotation_desc_2_event_id, \n",
    "        chunk_duration=chunk_duration)\n",
    "\n",
    "    # create a new event_id that unifies stages 3 and 4\n",
    "    event_id = {\n",
    "        'Sleep stage W': 1,\n",
    "        'Sleep stage 1': 2,\n",
    "        'Sleep stage 2': 3,\n",
    "        'Sleep stage 3/4': 4,\n",
    "        'Sleep stage R': 5}\n",
    "\n",
    "    tmax = 30. - 1. / raw.info['sfreq']  # tmax in included\n",
    "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
    "    epochs = mne.Epochs(raw=raw, events=events, picks=picks, preload=True,\n",
    "                        event_id=event_id, tmin=0., tmax=tmax, baseline=None)\n",
    "    \n",
    "    return epochs.get_data(), epochs.events[:, 2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "\n",
    "class EpochsDataset(Dataset):\n",
    "    \"\"\"Class to expose an MNE Epochs object as PyTorch dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs_data : np.ndarray\n",
    "        The epochs data, shape (n_epochs, n_channels, n_times).\n",
    "    epochs_labels : np.ndarray\n",
    "        The epochs labels, shape (n_epochs,)\n",
    "    subj_nb: None | int\n",
    "        Subject number.\n",
    "    rec_nb: None | int\n",
    "        Recording number.\n",
    "    transform : callable | None\n",
    "        The function to eventually apply to each epoch\n",
    "        for preprocessing (e.g. scaling). Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, epochs_data, epochs_labels, subj_nb=None, \n",
    "                 rec_nb=None, transform=None):\n",
    "        assert len(epochs_data) == len(epochs_labels)\n",
    "        self.epochs_data = epochs_data\n",
    "        self.epochs_labels = epochs_labels\n",
    "        self.subj_nb = subj_nb\n",
    "        self.rec_nb = rec_nb\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.epochs_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.epochs_data[idx], self.epochs_labels[idx]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        X = torch.as_tensor(X[None, ...])\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "def scale(X):\n",
    "    \"\"\"Standard scaling of data along the last dimention.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_channels, n_times)\n",
    "        The input signals.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_t : array, shape (n_channels, n_times)\n",
    "        The scaled signals.\n",
    "    \"\"\"\n",
    "    X -= np.mean(X, axis=1, keepdims=True)\n",
    "    return X / np.std(X, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extract windows from each recording, and wrap them into Pytorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply windowing and move to pytorch dataset\n",
    "all_datasets = [EpochsDataset(*extract_epochs(raw), subj_nb=raw.info['subject_info']['id'], \n",
    "                              rec_nb=raw.info['subject_info']['rec_id'], transform=scale) \n",
    "                for raw in raws]\n",
    "\n",
    "# Concatenate into a single dataset\n",
    "dataset = ConcatDataset(all_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Making train, valid and test splits\n",
    "\n",
    "Now that we have our preprocessed and windowed data, we can split it into the different sets that we will need: (1) the **training set** is used to learn the parameters of our ConvNet, (2) the **validation set** is used to monitor the training process and decide when to stop it, and (3) the **test set** is used to provide an estimate of generalization performance of our model.\n",
    "\n",
    "Here, we keep the first recording of subjects 0-9 for testing, and use the remaining recordings for training and validation.\n",
    "\n",
    "We define the following function to perform the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "\n",
    "def train_test_split(dataset, n_groups, split_by='subj_nb'):\n",
    "    \"\"\"Split torch dataset in train and test keeping n_groups out in test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : instance of ConcatDataset\n",
    "        The dataset to split.\n",
    "    n_groups : int\n",
    "        The number of groups to leave out.\n",
    "    split_by : 'subj_nb' | 'rec_nb'\n",
    "        Property to use to split dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ds_train : instance of Dataset\n",
    "        The training data.\n",
    "    ds_test : instance of Dataset\n",
    "        The testing data.\n",
    "    \"\"\"\n",
    "    groups = [getattr(ds, split_by) for ds in dataset.datasets]\n",
    "    train_idx, test_idx = next(\n",
    "        LeavePGroupsOut(n_groups).split(X=groups, groups=groups))\n",
    "\n",
    "    train_ds = ConcatDataset([dataset.datasets[i] for i in train_idx])\n",
    "    test_ds = ConcatDataset([dataset.datasets[i] for i in test_idx])\n",
    "        \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(87)\n",
    "np.random.seed(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjects_valid = max(1, int(len(dataset.datasets) * 0.1))\n",
    "train_ds, test_ds = train_test_split(dataset, n_subjects_valid, split_by='subj_nb')\n",
    "train_ds, valid_ds = train_test_split(train_ds, n_subjects_valid, split_by='subj_nb')\n",
    "\n",
    "print('Number of examples in each set')\n",
    "print(f'Training: {len(train_ds)}')\n",
    "print(f'Validation: {len(valid_ds)}')\n",
    "print(f'Test: {len(test_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we notice that the classes are imbalanced, i.e., there are a lot more of some classes than others. N2 is especially more prevalent than the other sleep stages during the night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_mapping = {0: 'W', 1: 'N1', 2: 'N2', 3: 'N3', 4: 'R'}\n",
    "y_train = pd.Series([y for _, y in train_ds]).map(classes_mapping)\n",
    "ax = y_train.value_counts().plot(kind='barh')\n",
    "ax.set_xlabel('Number of training examples');\n",
    "ax.set_ylabel('Sleep stage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to account for this imbalance during training is to give more weight to examples from rarer classes when computing the loss. We compute the weights with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing class weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "train_y = np.concatenate([ds.epochs_labels for ds in train_ds.datasets])\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_y), y=train_y)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the neural network\n",
    "\n",
    "In this section, we will define our ConvNet architecture. \n",
    "\n",
    "By default, we use the sleep staging architecture of Chambon et al. (2018), which looks something like this (adapted from Banville et al. 2020):\n",
    "\n",
    "![convnet](figs/convnet.png \"SleepStagerChambon2018\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input**, on the left, is a 30-s window of `C` channels. By default we have set `C` to 2 by selecting the 2 available EEG channels in Sleep Physionet above.\n",
    "\n",
    "The **output**, on the right, is a 5-dimensional vector where each dimension is matched to one of our 5 classes (W, N1, N2, N3 and R sleep stages).\n",
    "\n",
    "In between, we have a succession of convolutional layers, max pooling, and nonlinearities. The feature maps are finally flattened and passed through a fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SleepStagerChambon2018(nn.Module):\n",
    "    \"\"\"Sleep staging architecture from [1]_.\n",
    "    \n",
    "    Convolutional neural network for sleep staging described in [1]_.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_channels : int\n",
    "        Number of EEG channels.\n",
    "    sfreq : float\n",
    "        EEG sampling frequency.\n",
    "    n_conv_chs : int\n",
    "        Number of convolutional channels. Set to 8 in [1]_.\n",
    "    time_conv_size_s : float\n",
    "        Size of filters in temporal convolution layers, in seconds. Set to 0.5\n",
    "        in [1]_ (64 samples at sfreq=128).\n",
    "    max_pool_size_s : float\n",
    "        Max pooling size, in seconds. Set to 0.125 in [1]_ (16 samples at\n",
    "        sfreq=128).\n",
    "    n_classes : int\n",
    "        Number of classes.\n",
    "    input_size_s : float\n",
    "        Size of the input, in seconds.\n",
    "    dropout : float\n",
    "        Dropout rate before the output dense layer.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., &\n",
    "           Gramfort, A. (2018). A deep learning architecture for temporal sleep\n",
    "           stage classification using multivariate and multimodal time series.\n",
    "           IEEE Transactions on Neural Systems and Rehabilitation Engineering,\n",
    "           26(4), 758-769.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, sfreq, n_conv_chs=8, time_conv_size_s=0.5,\n",
    "                 max_pool_size_s=0.125, n_classes=5, input_size_s=30,\n",
    "                 dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        time_conv_size = int(time_conv_size_s * sfreq)\n",
    "        max_pool_size = int(max_pool_size_s * sfreq)\n",
    "        input_size = int(input_size_s * sfreq)\n",
    "        pad_size = time_conv_size // 2\n",
    "        self.n_channels = n_channels\n",
    "        len_last_layer = self._len_last_layer(\n",
    "            n_channels, input_size, max_pool_size, n_conv_chs)\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.spatial_conv = nn.Conv2d(1, n_channels, (n_channels, 1))\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, n_conv_chs, (1, time_conv_size), padding=(0, pad_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size)),\n",
    "            nn.Conv2d(\n",
    "                n_conv_chs, n_conv_chs, (1, time_conv_size),\n",
    "                padding=(0, pad_size)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1, max_pool_size))\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(len_last_layer, n_classes)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _len_last_layer(n_channels, input_size, max_pool_size, n_conv_chs):\n",
    "        return n_channels * (input_size // (max_pool_size ** 2)) * n_conv_chs\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
    "        \"\"\"\n",
    "        if self.n_channels > 1:\n",
    "            x = self.spatial_conv(x)\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.feature_extractor(x)\n",
    "        return self.fc(x.flatten(start_dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = raws[0].info['sfreq']  # Sampling frequency\n",
    "n_channels = raws[0].info['nchan']  # Number of channels\n",
    "\n",
    "model = SleepStagerChambon2018(n_channels, sfreq, n_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether a CUDA-enabled GPU is available, we can move the model to the GPU and perform the training there. This can enable significant speed-ups, but is not strictly required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Selected device \\'{device}\\'.')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and monitor network\n",
    "\n",
    "We are almost ready to finally train our ConvNet!\n",
    "\n",
    "We first need to define `DataLoader`s. `DataLoader` is a pytorch object that wraps a dataset and makes it easy to iterate over batches of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataloaders\n",
    "train_batch_size = 128  # Important hyperparameter\n",
    "valid_batch_size = 256  # Can be made as large as what fits in memory\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    train_ds, batch_size=train_batch_size, shuffle=True, num_workers=num_workers)\n",
    "loader_valid = DataLoader(\n",
    "    valid_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)\n",
    "loader_test = DataLoader(\n",
    "    test_ds, batch_size=valid_batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few functions to carry out our training and validation loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "def _do_train(model, loader, optimizer, criterion, device, metric):\n",
    "    # training loop\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "    for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "        batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "        y_true_all.append(batch_y.cpu().numpy())\n",
    "\n",
    "        train_loss[idx_batch] = loss.item()\n",
    "        \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "    \n",
    "    return np.mean(train_loss), perf\n",
    "        \n",
    "\n",
    "def _validate(model, loader, criterion, device, metric):\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = np.zeros(len(loader))\n",
    "    y_pred_all, y_true_all = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for idx_batch, (batch_x, batch_y) in enumerate(loader):\n",
    "            batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "            batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "            output = model.forward(batch_x)\n",
    "\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss[idx_batch] = loss.item()\n",
    "            \n",
    "            y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "            y_true_all.append(batch_y.cpu().numpy())\n",
    "            \n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    perf = metric(y_true, y_pred)\n",
    "\n",
    "    return np.mean(val_loss), perf\n",
    "\n",
    "\n",
    "def train(model, loader_train, loader_valid, optimizer, criterion, n_epochs, \n",
    "          patience, device, metric=None):\n",
    "    \"\"\"Training function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : instance of nn.Module\n",
    "        The model.\n",
    "    loader_train : instance of Sampler\n",
    "        The generator of EEG samples the model has to train on.\n",
    "        It contains n_train samples\n",
    "    loader_valid : instance of Sampler\n",
    "        The generator of EEG samples the model has to validate on.\n",
    "        It contains n_val samples. The validation samples are used to\n",
    "        monitor the training process and to perform early stopping\n",
    "    optimizer : instance of optimizer\n",
    "        The optimizer to use for training.\n",
    "    n_epochs : int\n",
    "        The maximum of epochs to run.\n",
    "    patience : int\n",
    "        The patience parameter, i.e. how long to wait for the\n",
    "        validation error to go down.\n",
    "    metric : None | callable\n",
    "        Metric to use to evaluate performance on the training and\n",
    "        validation sets. Defaults to balanced accuracy.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_model : instance of nn.Module\n",
    "        The model that led to the best prediction on the validation\n",
    "        dataset.\n",
    "    history : list of dicts\n",
    "        Training history (loss, accuracy, etc.)\n",
    "    \"\"\"\n",
    "    best_valid_loss = np.inf\n",
    "    best_model = copy.deepcopy(model)\n",
    "    waiting = 0\n",
    "    history = list()\n",
    "    \n",
    "    if metric is None:\n",
    "        metric = balanced_accuracy_score\n",
    "        \n",
    "    print('epoch \\t train_loss \\t valid_loss \\t train_perf \\t valid_perf')\n",
    "    print('-------------------------------------------------------------------')\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_loss, train_perf = _do_train(\n",
    "            model, loader_train, optimizer, criterion, device, metric=metric)\n",
    "        valid_loss, valid_perf = _validate(\n",
    "            model, loader_valid, criterion, device, metric=metric)\n",
    "        history.append(\n",
    "            {'epoch': epoch, \n",
    "             'train_loss': train_loss, 'valid_loss': valid_loss,\n",
    "             'train_perf': train_perf, 'valid_perf': valid_perf})\n",
    "        \n",
    "        print(f'{epoch} \\t {train_loss:0.4f} \\t {valid_loss:0.4f} '\n",
    "              f'\\t {train_perf:0.4f} \\t {valid_perf:0.4f}')\n",
    "\n",
    "        # model saving\n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(f'best val loss {best_valid_loss:.4f} -> {valid_loss:.4f}')\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            waiting = 0\n",
    "        else:\n",
    "            waiting += 1\n",
    "\n",
    "        # model early stopping\n",
    "        if waiting >= patience:\n",
    "            print(f'Stop training at epoch {epoch}')\n",
    "            print(f'Best val loss : {best_valid_loss:.4f}')\n",
    "            break\n",
    "\n",
    "    return best_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two critical pieces of the training process are the **optimizer** and the **criterion**.\n",
    "\n",
    "* The **optimizer** implements the parameter update procedure. Here, we use `Adam`, a popular adaptive gradient descent optimizer for deep neural networks.\n",
    "* The **criterion**, or loss function, is used to measure how well the neural network performs on an example. Here, we use the standard multiclass cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "criterion = CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now launch our training loop. The maxmium number of training epochs (or \"passes\" through the training set) is set with `n_epochs`. The `patience` hyperparameter controls how many epochs we will wait for before stopping the training process if there is no improvement on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "patience = 5\n",
    "\n",
    "best_model, history = train(\n",
    "    model, loader_train, loader_valid, optimizer, criterion, n_epochs, patience, \n",
    "    device, metric=cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the results of our training.\n",
    "\n",
    "First, the training curves show how the loss and accuracy improved across training epochs. We use [Cohen's kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) (instead of the standard accuracy) to better reflect performance under class imbalance and allow comparison with results from the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the learning curves\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "ax1 = history_df.plot(x='epoch', y=['train_loss', 'valid_loss'], marker='o')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2 = history_df.plot(x='epoch', y=['train_perf', 'valid_perf'], marker='o')\n",
    "ax2.set_ylabel('Cohen\\'s kappa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also measure the performance on the test set, which was not seen during training. This gives us a better estimate of the generalization performance of our ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test performance\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "y_pred_all, y_true_all = list(), list()\n",
    "for batch_x, batch_y in loader_test:\n",
    "    batch_x = batch_x.to(device=device, dtype=torch.float32)\n",
    "    batch_y = batch_y.to(device=device, dtype=torch.int64)\n",
    "    output = model.forward(batch_x)\n",
    "    y_pred_all.append(torch.argmax(output, axis=1).cpu().numpy())\n",
    "    y_true_all.append(batch_y.cpu().numpy())\n",
    "    \n",
    "y_pred = np.concatenate(y_pred_all)\n",
    "y_true = np.concatenate(y_true_all)\n",
    "rec_ids = np.concatenate(  # indicates which recording each example comes from\n",
    "    [[i] * len(ds) for i, ds in enumerate(test_ds.datasets)])\n",
    "\n",
    "test_bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "test_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f'Test balanced accuracy: {test_bal_acc:0.3f}')\n",
    "print(f'Test Cohen\\'s kappa: {test_kappa:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our 5-class problem, chance-level would correspond to 20% balanced accuracy or a Cohen's kappa of 0.0.\n",
    "\n",
    "To get a sense of what is possible, a recent model achieved a kappa of 0.814 on the Sleep Physionet data using a single EEG channel (10-fold cross-validation):\n",
    "\n",
    "> Phan, H., Chn, O. Y., Koch, P., Mertins, A., & De Vos, M. (2020). Xsleepnet: Multi-view sequential model for automatic sleep staging. arXiv preprint arXiv:2007.05492"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing results\n",
    "\n",
    "We further inspect the results in this section.\n",
    "\n",
    "We start by looking at the confusion matrix, which allows us to see which classes were easier or more difficult to classify for our ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, classes_mapping):\n",
    "    ticks = list(classes_mapping.keys())\n",
    "    tick_labels = classes_mapping.values()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(conf_mat, cmap='Reds')\n",
    "\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(tick_labels)\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_title('Confusion matrix')\n",
    "\n",
    "    for i in range(len(ticks)):\n",
    "        for j in range(len(ticks)):\n",
    "            text = ax.text(\n",
    "                j, i, conf_mat[i, j], ha='center', va='center', color='k')\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.05)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(conf_mat, classes_mapping);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of mistakes does the ConvNet seem to make? Is there a class that's often mistaken for another one?\n",
    "\n",
    "We can also visualize the predictions on a recording basis. This visualization is known as a \"hypnogram\". A hypnogram shows the evolution of sleep stages across an overnight recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hypnogram for one recording\n",
    "\n",
    "mask = rec_ids == 0  # pick a recording number\n",
    "\n",
    "t = np.arange(len(y_true[mask])) * 30 / 3600\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(t, y_true[mask], label='True')\n",
    "ax.plot(t, y_pred[mask], alpha=0.7, label='Predicted')\n",
    "ax.set_yticks([0, 1, 2, 3, 4])\n",
    "ax.set_yticklabels(['W', 'N1', 'N2', 'N3', 'R'])\n",
    "ax.set_xlabel('Time (h)')\n",
    "ax.set_title('Hypnogram')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the predictions of the ConvNet follow the groundtruth hypnogram? Is there any structure in the way mistakes are made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "You've now covered all the material of this tutorial!\n",
    "\n",
    "To test what you have learned, we recommend you identify a few key elements in the pipeline shown above, and play with them to try to improve the performance of your ConvNet. Here are a few ideas to get you started:\n",
    "- Increasing the training set size\n",
    "- Improving the architecture (you can look at recent sleep staging literature, or follow your intuition!)\n",
    "- Optimizing the training hyperparameters (learning rate, batch size, etc.)\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
